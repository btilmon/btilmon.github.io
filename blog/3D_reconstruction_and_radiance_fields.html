<!DOCTYPE html>



 <html>

<head>
  
  <title>Brevin Tilmon</title>
  <link rel="stylesheet" type="text/css" href="../style.css">
  <link rel="stylesheet" href="http://domain.tld/screen.css" type="text/css" media="Screen" />
  <link rel="stylesheet" href="http://domain.tld/mobile.css" type="text/css" media="handheld" />
  <!-- <link href="https://fonts.googleapis.com/css?family=Roboto:light" rel="stylesheet"> -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Nunito&display=swap" rel="stylesheet"> -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet"> -->
  <link rel="shortcut icon" href=""/> 

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
* {
  box-sizing: border-box;
}

/* Create two unequal columns that floats next to each other */
.column {
  float: left;
  padding-right: 2px;
  padding-bottom: 25px;
  height: 100%;
}

.left {
  width: 25%;
  padding-top: 4px;
}


.right {
  width: 75%;
}

.columnIm {
  float: left;
  padding-right: 2px;
  padding-bottom: 0px;
  height: 100%;
}

.leftIm {
  width: 20%;
  padding-top: 4px;
  padding-right: 20px;
}


.rightIm {
  width: 67%;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}
</style>

</head>



<body>
  <meta name="viewport" content="width=device-width, initial-scale=1">
<br><br>
<div class="container_blog" >  

<div class="container">

  <h2>3D Reconstruction and Radiance Fields</h1>
        <div style='margin-left:0px;'>
          
            <br><br><br>

            This is a collection of my notes on 3D Reconstruction and Radiance Fields I have referenced throughout my computer vision career. I plan on expanding it as the field evolves. The effectiveness of Radiance Fields compelled me to get all the geometric computer vision ideas in one place and hopefully explain them clearly. This blog was originally partially inspired by the revolutionary computer graphics blog <a href="https://www.scratchapixel.com/index.php?redirect">scratchapixel</a>. I may or may not do something similar for computer vision.

            <br><br><br>

            <h8><b>3D Reconstruction Background</b></h8> <br>
            Two end games of computer vision and graphics are obtaining a constantly updated open source photorealistic 3D reconstruction (mapping) of the world and a fast method for high quality object capture. High quality world maps enable self-driving cars and drones to localize themselves against the map, phones/VR/AR devices to localize themselves and project realistic graphics, and the development of motion planning based on ground truth from the existing map. High quality object capture promises to revolutionize several industries and decentralize technology. In a hypothetical future, as the real world is observed by our cameras, the images could be sent to the cloud and the world map continuously updated. At the same time, the world map could be continuously refined with <a href="https://en.wikipedia.org/wiki/Bundle_adjustment">bundle adjustment</a>. Google, among the computer vision and graphics communities, has been thinking about this since the early 2000’s with Google Street View and Waymo cars. I assume Google uses Street View to localize their cars and for ground truthing against the world map. They drive cars around with a sensor suite, and run Visual <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">SLAM</a> (aka structure from motion) on the world to give us street view, <a href="http://ceres-solver.org/users.html">ceres solver</a> is what they use to optimize the world map with bundle adjustment. Visual SLAM gives us a RGBD point cloud that is meshed together with techniques such as <a href="https://hhoppe.com/poissonrecon.pdf">Poisson Reconstruction</a>, this meshed representation is the map.

            <br><br>

            A world map is evidently not the secret ingredient to winning self driving cars, although I think we all could use a little redefining what winning self driving cars looks like, but it does let you know where the robot is and what is globally around the robot, enabling several self driving car companies to operate successfully in mapped regions (Cadillac supercruise, Cruise, Waymo, etcetera), and rovers to map planetary bodies. <a href="https://comma.ai/">Comma.ai</a> does not publicly state they use maps, and instead markets their end to end camera-only system, which is truly revolutionary and similar to Tesla Autopilot except open source (woo!). Since the world map updates are slow, we can use relative depth estimation techniques. Basically, while we wait for the world map to update we can, in the meantime, determine what is around the robot with per frame relative depth maps. These depth maps can optionally be integrated into the metric world map through a Truncated Signed Distance Function representation (<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/kinectfusion-uist-comp.pdf">Kinect Fusion</a>), among other techniques. Relative depth estimation can be computed with stereo, lidar, monocular depth estimation, etcetera. Relative depth estimation is good for real time obstacle avoidance etcetera but remember, if we have a posed camera provided by SLAM, we can always project the world map into this camera to get a depth map. It is just that the world map may not be accurate since it takes a while to update. 


            <br><br>

            Two paradigms have been established. One paradigm involves a photorealistic map of the world, and the other paradigm is real time depth estimation. A groundbreaking hybrid of these two paradigms was introduced with <a href="https://vision.in.tum.de/research/vslam/lsdslam">LSD-SLAM</a>. This is a real time direct visual slam system that can give semi-dense depth maps and world maps on CPU. No feature detection or matching required, just advanced math with <a href="https://gtsam.org/">factor graphs</a> for scaling. However, for real time depth estimation in the case of “am I about to hit something” I believe relative depth estimation techniques (stereo, lidar, etcetera) have a formidable spot. See these pages on past and modern scalable solutions to visual SLAM [<a href="https://cmsc426.github.io/sfm/">1</a>, <a href="https://cmsc426.github.io/gtsam/">2</a>].

            <br><br><br>

            <h8><b>3D Reconstruction Breakthrough:</b></h8> <br>
            <h8><b>Neural Radiance Fields</b></h8> <br>
            I am now going to focus on getting a photorealistic map. We have all heard of the metaverse, and one version of the metaverse is this photorealistic world map with optional graphics included. The metaverse does not solely include a world map, it also entails object capture, which is getting a 3D model of an object which can then be included alongside a map, it is just a 3D model that could be rendered in a video game for example. Neural Radiance Fields (<a href="https://arxiv.org/pdf/2003.08934.pdf">NeRF</a>) have redefined how to best obtain a photorealistic map. Up to now, photorealistic maps were obtained with visual SLAM (or SfM, MVS, Photometric Stereo, etecetera) that gives us RGBD point clouds that were then meshed with a technique such as Poisson Reconstruction to get a map. This RGBD representation is a surface level representation that bakes in the myriad light transport effects. For example, scattering, diffusion, and caustics are superimposed onto each other and we only get the resulting RGB pixel value. This limits how photorealistic our reconstruction can be because when we simulate synthetic lighting, the reconstruction does not accurately reflect the light. This is why <a href="https://www.scratchapixel.com/lessons/3d-basic-rendering/phong-shader-BRDF">Phong Shading</a> is the de facto standard in computer graphics for video game rendering since the 3D models in the video game typically cannot have more than a RGBD representation due to computation constraints. For more on shading (Bidirectional Reflectance Distribution Function) and computer graphics, see <a href="https://www.scratchapixel.com/index.php?redirect">scratchapixel</a>.

            <br><br>

            The key insight of NeRF is representing scenes as a volume instead of a RGBD surface representation for view synthesis (aka projecting a photorealistic map into unseen views). This is based on light fields and volume rendering. Given posed input images from SLAM, we can train a Multi Layer Perceptron (!!!) to predict a color and volume density for a desired pose. By integrating along a ray through the volume we can get an expected ray termination based on estimated volume density and use this as a depth value. So, instead of running SLAM on a collection of images to get a RGBD representation, we can use NeRF to obtain a volumetric representation that gives a more physically accurate RGBD representation. Once we have the per view depth maps from NeRF we must still backproject the depth maps into a point cloud and mesh them as usual. Caveats: input images must be posed with SLAM first, takes a day to train, Convergence relies on quality coverage of the object. For offline object capture and world mapping though these caveats are not really caveats and are not too different from existing 3D reconstruction pipelines in terms of heuristics. I can now train a NeRF MLP to get a more photorealistic mesh compared to SLAM. 

            <br><br>

            Google recently ran NeRF on Street View images for an extremely accurate world map (<a href="https://urban-radiance-fields.github.io/">Urban Radiance Fields</a>). Urban Radiance Fields is one of the reasons I wrote this blog since until now I assumed NeRF was mostly an object capture technique with limited robotics applications, this changed my perspective. I probably sound like a broken record, but, instead of running SLAM on the input images + X to obtain a map, they run NeRF and regularize with ground truth lidar to represent the world as a volume. This gives highly accurate depth and depth for unseen views. I think NeRF will drastically improve SLAM techniques: collecting images with a drone, phone, or VR headset and then offline using NeRF will give much more accurate maps than SLAM in some instances. For example a drone can inspect a bridge and run NeRF for offline inspection. You might be wondering what happens if something in the world moves as this invalidates the map and the NeRF MLP must be retrained. <a href="https://arxiv.org/pdf/2104.06405.pdf">Bundle Adjusting Radiance Fields</a> approaches this problem by using bundle adjustment to allow for imperfect posed input images, possibly allowing for a continuously updated map in the future, but as of now dynamic scenes are the main limitation of radiance field techniques for robotics applications in my opinion, although dynamic scenes might simply be the wrong problem for radiance fields.  

            <br><br><br>

            <h8><b>Rethinking NeRF:</b></h8> <br>
            <h8><b>Plenoxels</b></h8> <br>
            Recently, the paper “<a href="https://alexyu.net/plenoxels/?s=09">Plenoxels: Radiance Fields without Neural Networks</a>" replaced the volume representation of NeRF with a spherical harmonic voxel representation. The spherical harmonic coefficients are optimized during “training” and interpolated to give the predicted color and density. Plenoxels can train in 8 minutes compared to 24 hours of NeRF. The volume rendering ideas for the loss are the same but the representation is now voxels. The downside is the trained model of Plenoxels is larger due to the voxel representation but again in many applications this is probably fine. We can now train a plenoxel model to give a 3D model in reasonable time, this truly revolutionizes object capture. I have not seen an Urban Radiance Field extension to plenoxels but I imagine it will come, revolutionizing robotics as well. <a href="https://lumalabs.ai/">LUMA AI</a> is a new startup that just hired Alex Yu, one of the Plenoxel authors. It seems they are marketing themselves as a quick object capture service, think of a more accurate COLMAP in an app, I am looking forward to seeing what they build.

            <br><br><br>

            <h8><b>Conclusion</b></h8> <br>
            
            I assume that real time depth estimation and SLAM will remain supreme in real time robotics mapping applications due to the maturity of the algorithms and having to retrain the radiance fields for dynamic scenes, though I already see work handling <a href="https://vcai.mpi-inf.mpg.de/projects/nonrigid_nerf/">dynamic scenes</a> so this may change. SLAM allows for easier map updating than radiance fields as of today, but the recent work in <a href="https://edgarsucar.github.io/iMAP/">iMAP: Implicit Mapping and Positioning in Real-Time</a> questions this assumption. Urban Radiance Fields suggests it is probably a good idea to have your robot collect images and then train a radiance field offline if your application allows it. Real time depth estimation still seems strongly positioned for certain robotics applications, albeit maybe computed in a different representation than today, and maybe eliminated altogether if dense map updating can happen at a similar frame rate for certain applications. In conclusion, Radiance Fields have positioned themselves as a new technique for computer vision researchers and practicioners to try for 3D Reconstruction.

            <br><br><br>

            Brevin Tilmon <br>
            <a href="https://btilmon.github.io/">https://btilmon.github.io/</a> <br>
            January 12, 2022


            <br><br><br>

            <h8><b>Additional Resources</b></h8> <br>

            <ul>
                <li>Approachable introduction to NeRF by Frank Dellaert [<a href="https://dellaert.github.io/NeRF/">link</a>]</li>
                <li>Mathematical Foundations of Computer Vision and Computer Graphics [<a href="http://lucaballan.altervista.org/teaching.html">link</a>]</li>
                <li>Light fields</li>
                <ul>
                    <li>Original light field paper [<a href="https://graphics.stanford.edu/papers/light/light-lores-corrected.pdf">link</a>]</li>
                    <li>Light field photography with a hand-held plenoptic camera [<a href="https://hal.archives-ouvertes.fr/hal-02551481/document">link</a>]</li>
                </ul>
                <li>Computer Vision: Algorithms and Applications, 2nd Edition [<a href="https://szeliski.org/Book/">link</a>]</li>
            </ul>

            
        </div>  
</div>




</body>
</html>
